{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the EdgeTapor to a simpler version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import random\n",
    "import pickle   \n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from thop import profile\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained model weights\n",
    "trained_model_weight = None\n",
    "\n",
    "trained_model_weight = 'NanoTapor_files/NanoTapor_model_weights/44_20243715330_tm2_adaptor.pth'\n",
    "\n",
    "if not os.path.exists(trained_model_weight):\n",
    "    print('Model weights found')\n",
    "    trained_model_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = 'NanoTapor_files/'\n",
    "if not os.path.exists(saved_path):\n",
    "    os.makedirs(saved_path)\n",
    "\n",
    "\n",
    "# k=8 # the final edge model size is: unsigned int converted_model_tflite_len = 124288;  params: 115791.00 FLOPs: 715608.00\n",
    "# last_conv_channel = 48  #should bigger than 64, when exteme_light=True, last_conv_channel doesn't work\n",
    "# # RAM:   [=====     ]  46.3% (used 151828 bytes from 327680 bytes)\n",
    "# # Flash: [=         ]  13.8% (used 460853 bytes from 3342336 bytes)\n",
    "# # Inference time delay (millis): 371.000000\n",
    "\n",
    "####### finally used ################\n",
    "k=16 # the final edge model size is: unsigned int converted_model_tflite_len = 165048;  params: 156479.00 FLOPs: 647856.00\n",
    "last_conv_channel = 32  #should bigger than 64, when exteme_light=True, last_conv_channel doesn't work\n",
    "# RAM:   [======    ]  58.8% (used 192596 bytes from 327680 bytes)\n",
    "# Flash: [==        ]  15.0% (used 501621 bytes from 3342336 bytes)\n",
    "# Inference time delay (millis): 313.000000\n",
    "\n",
    "# k=16 # the final edge model size is: unsigned int converted_model_tflite_len = 198680;  params: 189895.00 FLOPs: 735408.00\n",
    "# last_conv_channel = 40  #should bigger than 64, when exteme_light=True, last_conv_channel doesn't work\n",
    "# RAM:   [=======   ]  69.0% (used 226228 bytes from 327680 bytes)\n",
    "# Flash: [==        ]  16.0% (used 535253 bytes from 3342336 bytes)\n",
    "# Inference time delay (millis): 347.000000\n",
    "\n",
    "# k=12 # the final edge model size is: unsigned int converted_model_tflite_len = 178296;  params: 169551.00 FLOPs: 769284.00\n",
    "# last_conv_channel = 48  #should bigger than 64, when exteme_light=True, last_conv_channel doesn't work\n",
    "# # RAM:   [======    ]  62.8% (used 205844 bytes from 327680 bytes)\n",
    "# # Flash: [==        ]  15.4% (used 514869 bytes from 3342336 bytes)\n",
    "# # Inference time delay (millis): 376.000000\n",
    "\n",
    "# k=16 # the final edge model size is: unsigned int converted_model_tflite_len = 232312;  params: 223311.00 FLOPs: 822960.00\n",
    "# last_conv_channel = 48  #should bigger than 64, when exteme_light=True, last_conv_channel doesn't work\n",
    "# # RAM:   [========  ]  79.3% (used 259860 bytes from 327680 bytes)\n",
    "# # Flash: [==        ]  17.0% (used 568885 bytes from 3342336 bytes)\n",
    "# # Inference time delay (millis): 381.000000\n",
    "\n",
    "# k=16 # the final edge model size is: unsigned int converted_model_tflite_len = 299576;  params: 290143.00 FLOPs: 998064.00\n",
    "# last_conv_channel = 64  #should bigger than 64, when exteme_light=True, last_conv_channel doesn't work\n",
    "# # region `dram0_0_seg' overflowed by 18472 bytes\n",
    "\n",
    "feat_dim = 21*k   # this parameter to control the size of the tapor edge model\n",
    "model_save_path = \"NanoTapor_files/testing_edge_strudent_pytorch.pth\"  # pytorch model saved path\n",
    "onnx_saved_path = \"NanoTapor_files/testing_edge_tapor_v12.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 63])\n",
      "torch.Size([1, 336])\n",
      "params: 156479.00\n",
      "FLOPs: 647856.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "EdgeTaporStudent                         [1, 63]                   --\n",
       "├─Sequential: 1-1                        [1, 336]                  --\n",
       "│    └─Conv2d: 2-1                       [1, 8, 24, 32]            80\n",
       "│    └─ReLU: 2-2                         [1, 8, 24, 32]            --\n",
       "│    └─MaxPool2d: 2-3                    [1, 8, 12, 16]            --\n",
       "│    └─Conv2d: 2-4                       [1, 16, 12, 16]           1,168\n",
       "│    └─ReLU: 2-5                         [1, 16, 12, 16]           --\n",
       "│    └─MaxPool2d: 2-6                    [1, 16, 6, 8]             --\n",
       "│    └─Conv2d: 2-7                       [1, 32, 6, 8]             4,640\n",
       "│    └─ReLU: 2-8                         [1, 32, 6, 8]             --\n",
       "│    └─MaxPool2d: 2-9                    [1, 32, 3, 4]             --\n",
       "│    └─Flatten: 2-10                     [1, 384]                  --\n",
       "│    └─Linear: 2-11                      [1, 336]                  129,360\n",
       "│    └─ReLU: 2-12                        [1, 336]                  --\n",
       "├─Sequential: 1-2                        [1, 63]                   --\n",
       "│    └─Linear: 2-13                      [1, 63]                   21,231\n",
       "==========================================================================================\n",
       "Total params: 156,479\n",
       "Trainable params: 156,479\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.66\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.09\n",
       "Params size (MB): 0.63\n",
       "Estimated Total Size (MB): 0.72\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class EdgeTaporStudent(nn.Module):\n",
    "    def __init__(self, feat_dim =21*48, last_conv_channel=128):\n",
    "        super(EdgeTaporStudent, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, last_conv_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(last_conv_channel*3*4, feat_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 21*3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, thermal_map):\n",
    "        feat = self.encoder(thermal_map)\n",
    "        pose = self.decoder(feat)\n",
    "        return pose, feat\n",
    "\n",
    "x = torch.randn(1, 1, 24, 32)\n",
    "edge_tapor_strudent = EdgeTaporStudent(feat_dim, last_conv_channel)\n",
    "a, b = edge_tapor_strudent(x)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "# random intit a model and save it for code testing\n",
    "torch.save(edge_tapor_strudent.state_dict(), model_save_path)\n",
    "\n",
    "flops, params = profile(edge_tapor_strudent, inputs=(x,), verbose=False)\n",
    "print(\"params: {:.2f}\".format(params))\n",
    "print(\"FLOPs: {:.2f}\".format(flops))\n",
    "\n",
    "summary(edge_tapor_strudent, input_size=(1, 1, 24, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load trained model weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EdgeTapor(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Flatten(start_dim=1, end_dim=-1)\n",
       "    (10): Linear(in_features=384, out_features=336, bias=True)\n",
       "    (11): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=336, out_features=63, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we delete the feat output for the version that deploy on edge device\n",
    "class EdgeTapor(nn.Module):\n",
    "    def __init__(self, feat_dim =21*48,last_conv_channel=128):\n",
    "        super(EdgeTapor, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, last_conv_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(last_conv_channel*3*4, feat_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 21*3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, thermal_map):\n",
    "        feat = self.encoder(thermal_map)\n",
    "        pose = self.decoder(feat)\n",
    "        return pose\n",
    "\n",
    "\n",
    "# transfor edge student model to the edge model, just for code testing\n",
    "edge_tapor = EdgeTapor(feat_dim, last_conv_channel)\n",
    "\n",
    "if trained_model_weight is not None:\n",
    "    edge_tapor.load_state_dict(torch.load(trained_model_weight))\n",
    "    print('Load trained model weight')\n",
    "else:\n",
    "    edge_tapor.load_state_dict(torch.load(model_save_path))\n",
    "edge_tapor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%thermal_map : Float(1, 1, 24, 32, strides=[768, 768, 32, 1], requires_grad=0, device=cpu),\n",
      "      %encoder.0.weight : Float(8, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %encoder.0.bias : Float(8, strides=[1], requires_grad=1, device=cpu),\n",
      "      %encoder.3.weight : Float(16, 8, 3, 3, strides=[72, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %encoder.3.bias : Float(16, strides=[1], requires_grad=1, device=cpu),\n",
      "      %encoder.6.weight : Float(32, 16, 3, 3, strides=[144, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %encoder.6.bias : Float(32, strides=[1], requires_grad=1, device=cpu),\n",
      "      %encoder.10.weight : Float(336, 384, strides=[384, 1], requires_grad=1, device=cpu),\n",
      "      %encoder.10.bias : Float(336, strides=[1], requires_grad=1, device=cpu),\n",
      "      %decoder.0.weight : Float(63, 336, strides=[336, 1], requires_grad=1, device=cpu),\n",
      "      %decoder.0.bias : Float(63, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/encoder/encoder.0/Conv_output_0 : Float(1, 8, 24, 32, strides=[6144, 768, 32, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/encoder/encoder.0/Conv\"](%thermal_map, %encoder.0.weight, %encoder.0.bias), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.conv.Conv2d::encoder.0 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/encoder/encoder.1/Relu_output_0 : Float(1, 8, 24, 32, strides=[6144, 768, 32, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/encoder/encoder.1/Relu\"](%/encoder/encoder.0/Conv_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.activation.ReLU::encoder.1 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/encoder/encoder.2/MaxPool_output_0 : Float(1, 8, 12, 16, strides=[1536, 192, 16, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/encoder/encoder.2/MaxPool\"](%/encoder/encoder.1/Relu_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.pooling.MaxPool2d::encoder.2 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/functional.py:791:0\n",
      "  %/encoder/encoder.3/Conv_output_0 : Float(1, 16, 12, 16, strides=[3072, 192, 16, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/encoder/encoder.3/Conv\"](%/encoder/encoder.2/MaxPool_output_0, %encoder.3.weight, %encoder.3.bias), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.conv.Conv2d::encoder.3 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/encoder/encoder.4/Relu_output_0 : Float(1, 16, 12, 16, strides=[3072, 192, 16, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/encoder/encoder.4/Relu\"](%/encoder/encoder.3/Conv_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.activation.ReLU::encoder.4 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/encoder/encoder.5/MaxPool_output_0 : Float(1, 16, 6, 8, strides=[768, 48, 8, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/encoder/encoder.5/MaxPool\"](%/encoder/encoder.4/Relu_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.pooling.MaxPool2d::encoder.5 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/functional.py:791:0\n",
      "  %/encoder/encoder.6/Conv_output_0 : Float(1, 32, 6, 8, strides=[1536, 48, 8, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/encoder/encoder.6/Conv\"](%/encoder/encoder.5/MaxPool_output_0, %encoder.6.weight, %encoder.6.bias), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.conv.Conv2d::encoder.6 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/modules/conv.py:456:0\n",
      "  %/encoder/encoder.7/Relu_output_0 : Float(1, 32, 6, 8, strides=[1536, 48, 8, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/encoder/encoder.7/Relu\"](%/encoder/encoder.6/Conv_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.activation.ReLU::encoder.7 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/functional.py:1471:0\n",
      "  %/encoder/encoder.8/MaxPool_output_0 : Float(1, 32, 3, 4, strides=[384, 12, 4, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/encoder/encoder.8/MaxPool\"](%/encoder/encoder.7/Relu_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.pooling.MaxPool2d::encoder.8 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/functional.py:791:0\n",
      "  %/encoder/encoder.9/Flatten_output_0 : Float(1, 384, strides=[384, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/encoder/encoder.9/Flatten\"](%/encoder/encoder.8/MaxPool_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.flatten.Flatten::encoder.9 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/modules/flatten.py:47:0\n",
      "  %/encoder/encoder.10/Gemm_output_0 : Float(1, 336, strides=[336, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/encoder/encoder.10/Gemm\"](%/encoder/encoder.9/Flatten_output_0, %encoder.10.weight, %encoder.10.bias), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.linear.Linear::encoder.10 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/encoder/encoder.11/Relu_output_0 : Float(1, 336, strides=[336, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/encoder/encoder.11/Relu\"](%/encoder/encoder.10/Gemm_output_0), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::encoder/torch.nn.modules.activation.ReLU::encoder.11 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/functional.py:1471:0\n",
      "  %pose : Float(1, 63, strides=[63, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/decoder/decoder.0/Gemm\"](%/encoder/encoder.11/Relu_output_0, %decoder.0.weight, %decoder.0.bias), scope: __main__.EdgeTapor::/torch.nn.modules.container.Sequential::decoder/torch.nn.modules.linear.Linear::decoder.0 # /home/zhangxie/anaconda3/envs/ira_hand/lib/python3.8/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%pose)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch_input = torch.randn(1, 1, 24, 32)\n",
    "input_names = [ \"thermal_map\" ]\n",
    "output_names = [ \"pose\" ]\n",
    "torch.onnx.export(edge_tapor, torch_input, onnx_saved_path, verbose=True, input_names=input_names, output_names=output_names, opset_version=12)\n",
    "\n",
    "# related materials online:\n",
    "# https://github.com/onnx/tutorials/blob/main/tutorials/VersionConversion.md\n",
    "# https://onnxruntime.ai/docs/reference/compatibility.html\n",
    "\n",
    "\n",
    "# check the generated onnx model\n",
    "##################################\n",
    "\n",
    "import onnx\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "# Load the ONNX model\n",
    "model = onnx.load(onnx_saved_path)\n",
    "onnx.checker.check_model(model)\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_saved_path)\n",
    "channels =1\n",
    "height = 24\n",
    "width = 32\n",
    "batch_size = 1\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {'thermal_map': np.random.randn(batch_size, channels, height, width).astype(np.float32)}\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira_hand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
