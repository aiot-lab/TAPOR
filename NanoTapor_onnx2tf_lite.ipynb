{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The related tutorial online is [here](https://github.com/sithu31296/PyTorch-ONNX-TFLite) with the env:\n",
    "\n",
    "```shell\n",
    "tensorflow==2.4.1\n",
    "onnx==1.8.0\n",
    "onnx-tf==1.7.0\n",
    "```\n",
    "\n",
    "Howerver, based on our test, to successfully convert the model to tensorflow lite, the following requirements are needed:\n",
    "\n",
    "```shell\n",
    "tensorflow==2.11.0\n",
    "onnx==1.14.1\n",
    "onnx-tf==1.10.0\n",
    "```\n",
    "\n",
    "where the onnx-tf can be installed by:\n",
    "    \n",
    "```shell\n",
    "git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:48:49.393933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 15:48:49.594072: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:49.594108: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-07 15:48:50.438204: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:50.438296: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:50.438303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"NanoTapor_files/testing_edge_tapor_v12.onnx\"\n",
    "tf_model_path = 'NanoTapor_files/testing_edge_tapor_tf'\n",
    "tflite_model_path = 'NanoTapor_files/testing_edge_tapor_tf.tflite'\n",
    "tflite_quant_model_path = 'NanoTapor_files/testing_edge_tapor_tf_quant.tflite'\n",
    "cali_data_path = 'NanoTapor_files/cali_dataset/x_cal.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:48:52.273159: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:52.273252: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:52.273299: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:52.273351: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:52.378370: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:52.378444: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-03-07 15:48:52.378463: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-03-07 15:48:52.379961: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhangxie/anaconda3/envs/ira_hand_edge/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: EdgeTapor_files/testing_edge_tapor_tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: EdgeTapor_files/testing_edge_tapor_tf/assets\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnx_model_path)\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_rep.export_graph(tf_model_path)\n",
    "\n",
    "# USING COMMAND LINE !!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# python -m tf2onnx.convert --saved-model edge_tapor_toy_tf --opset 12 --output model.onnx\n",
    "# https://github.com/onnx/tensorflow-onnx#cli-reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF mdoel inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.saved_model.load(tf_model_path)\n",
    "tf_model.trainable = False\n",
    "batch_size = 1\n",
    "channels = 1\n",
    "height = 24\n",
    "width = 32\n",
    "input_tensor = tf.random.uniform([batch_size, channels, height, width])\n",
    "out = tf_model(**{'thermal_map': input_tensor})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF to TF lite (can skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:48:56.587736: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2024-03-07 15:48:56.587784: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2024-03-07 15:48:56.588455: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: EdgeTapor_files/testing_edge_tapor_tf\n",
      "2024-03-07 15:48:56.589173: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2024-03-07 15:48:56.589187: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: EdgeTapor_files/testing_edge_tapor_tf\n",
      "2024-03-07 15:48:56.597453: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2024-03-07 15:48:56.597894: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2024-03-07 15:48:56.612361: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: EdgeTapor_files/testing_edge_tapor_tf\n",
      "2024-03-07 15:48:56.618784: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 30329 microseconds.\n",
      "2024-03-07 15:48:56.634810: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-07 15:48:56.680823: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2111] Estimated count of arithmetic ops: 1.318 M  ops, equivalently 0.659 M  MACs\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# TFlite to onnx:\n",
    "# python -m tf2onnx.convert --opset 16 --tflite edge_tapor_toy_tf.tflite --output model_tflt2onnx.onnx\n",
    "# https://github.com/onnx/tensorflow-onnx#cli-reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the inference of tf lite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.4503378e-02  1.3470452e-01  5.8611584e-01 -7.1140967e-02\n",
      "   6.6913575e-02  5.0813520e-01 -6.0046475e-02  2.1431398e-02\n",
      "   5.0056887e-01 -4.8851054e-02  4.7152376e-04  4.9844366e-01\n",
      "  -4.9112290e-02  2.3369812e-03  5.0561678e-01 -8.0019414e-02\n",
      "   2.1016648e-02  5.0244939e-01 -9.6104048e-02 -5.2059989e-02\n",
      "   5.2891731e-01 -1.1857785e-01 -1.1511430e-01  5.2633607e-01\n",
      "  -1.3732748e-01 -1.5091980e-01  5.1979047e-01 -6.6179238e-02\n",
      "   1.6802894e-02  5.3996491e-01 -6.9964245e-02 -6.3421533e-02\n",
      "   5.3095233e-01 -8.0199912e-02 -1.2003826e-01  5.2029496e-01\n",
      "  -8.8223569e-02 -1.4944796e-01  5.0984728e-01 -5.0553188e-02\n",
      "   3.9346080e-02  5.5174071e-01 -4.2454716e-02 -5.1399237e-03\n",
      "   5.1394039e-01 -3.3817690e-02  2.7392061e-02  4.8997167e-01\n",
      "  -4.8188772e-02  4.8625272e-02  4.9969542e-01 -2.9548792e-02\n",
      "   4.5900710e-02  5.4310453e-01 -3.1528011e-02  5.4170405e-03\n",
      "   5.2833837e-01 -2.3911757e-02  1.8290281e-02  5.2012628e-01\n",
      "  -3.8952839e-02  2.5339417e-02  5.2101862e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF to TF lite with integer quantization\n",
    "\n",
    "Integer with float fallback (using default float input/output)\n",
    "The model is in integer but use float operators when they don't have an integer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 15:48:56.840060: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2024-03-07 15:48:56.840110: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2024-03-07 15:48:56.840277: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: EdgeTapor_files/testing_edge_tapor_tf\n",
      "2024-03-07 15:48:56.841070: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2024-03-07 15:48:56.841086: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: EdgeTapor_files/testing_edge_tapor_tf\n",
      "2024-03-07 15:48:56.842850: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2024-03-07 15:48:56.856285: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: EdgeTapor_files/testing_edge_tapor_tf\n",
      "2024-03-07 15:48:56.863016: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 22737 microseconds.\n",
      "2024-03-07 15:48:56.907368: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2111] Estimated count of arithmetic ops: 1.318 M  ops, equivalently 0.659 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EdgeTapor_files/testing_edge_tapor_tf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n",
      "2024-03-07 15:48:57.314325: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2111] Estimated count of arithmetic ops: 1.318 M  ops, equivalently 0.659 M  MACs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "165048"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf_model_path)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "with open(cali_data_path, 'rb') as f:\n",
    "    input = pickle.load(f)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for i in range(1000):\n",
    "        temp = input[i]\n",
    "        yield [temp[np.newaxis,...]]\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "tflite_quant_model = converter.convert()\n",
    "# converter.inference_input_type = tf.int8\n",
    "# converter.inference_output_type = tf.int8\n",
    "open(tflite_quant_model_path, \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting the quantized tflite model to hex file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is using the linux command to convert the model into hex file:\n",
    "```\n",
    "xxd -i model.tflite > EdgeTapor_files/model_data.cc\n",
    "```\n",
    "Then we can use the model_data.cc in the arduino code.\n",
    "\n",
    "If encounter the error: zsh: command not found: xxd, then we can use the following command to install xxd:\n",
    "```\n",
    "sudo apt-get install xxd\n",
    "```\n",
    "If the error still exists, then try this in a linux machine locally. OR use sudo:\n",
    "```\n",
    "sudo xxd -i model.tflite > EdgeTapor_files/model_data.cc\n",
    "```\n",
    "e.g.,\n",
    "```\n",
    "sudo xxd -i EdgeTapor_files/testing_edge_tapor_tf_quant.tflite > EdgeTapor_files/testing_model_data.cc\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira_hand_edge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
